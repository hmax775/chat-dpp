import streamlit as st
import google.generativeai as genai
import os
from dotenv import load_dotenv
import glob

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ©
load_dotenv()

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="Ø´Ø§Øª Ø¨ÙˆØª Ø§Ù„Ø°ÙƒÙŠ",
    page_icon="ğŸ¤–",
    layout="centered"
)

# Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
st.title("ğŸ¤– Ø´Ø§Øª Ø¨ÙˆØª Ø§Ù„Ø°ÙƒÙŠ")
st.markdown("Ø§Ø³Ø£Ù„Ù†ÙŠ Ø£ÙŠ Ø³Ø¤Ø§Ù„ ÙˆØ³Ø£Ø¬ÙŠØ¨Ùƒ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ©!")

# Ø¥Ø¹Ø¯Ø§Ø¯ API Key
def setup_api_key():
    api_key = st.sidebar.text_input("Ø£Ø¯Ø®Ù„ Gemini API Key:", type="password")
    if api_key:
        os.environ['GEMINI_API_KEY'] = api_key
        return api_key
    return None

# ØªØ­Ù…ÙŠÙ„ ÙˆÙ‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ©
def load_text_files():
    text_data = ""
    text_files = glob.glob("data/*.txt")
    
    if not text_files:
        st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª Ù†ØµÙŠØ© ÙÙŠ Ù…Ø¬Ù„Ø¯ 'data/'")
        return ""
    
    for file_path in text_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                text_data += f"\n\n--- Ù…Ø­ØªÙˆÙ‰ Ù…Ù„Ù {os.path.basename(file_path)} ---\n"
                text_data += file.read()
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù {file_path}: {str(e)}")
    
    return text_data

# ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Gemini
def initialize_model(api_key):
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-pro')
        return model
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)}")
        return None

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„ÙØ§Øª
def generate_response(model, context, question):
    prompt = f"""
    Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ©:
    
    {context}
    
    Ø§Ù„Ø³Ø¤Ø§Ù„: {question}
    
    ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ© Ø£Ø¹Ù„Ø§Ù‡.
    Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù†ØµØŒ Ù‚Ù„ Ø¨ÙƒÙ„ Ø£Ø¯Ø¨ Ø£Ù†Ùƒ Ù„Ø§ ØªÙ…Ù„Ùƒ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØ§ÙÙŠØ©.
    Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø§ Ù„Ù… ÙŠØ·Ù„Ø¨ Ù…Ù†Ùƒ ØºÙŠØ± Ø°Ù„Ùƒ.
    """
    
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯: {str(e)}"

# Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
    st.sidebar.header("Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    api_key = setup_api_key()
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ©
    st.sidebar.subheader("Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©")
    text_files = glob.glob("data/*.txt")
    for file in text_files:
        st.sidebar.write(f"ğŸ“„ {os.path.basename(file)}")
    
    if not api_key:
        st.info("ğŸ‘ˆ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Gemini API Key ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        return
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ©..."):
        context_data = load_text_files()
    
    if not context_data:
        return
    
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = initialize_model(api_key)
    if not model:
        return
    
    # Ø¹Ø±Ø¶ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    if question := st.chat_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§..."):
        # Ø¥Ø¶Ø§ÙØ© Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ù„Ù‰ Ø§Ù„Ø³Ø¬Ù„
        st.session_state.messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(question)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
        with st.chat_message("assistant"):
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø±Ø¯..."):
                response = generate_response(model, context_data, question)
                st.markdown(response)
        
        # Ø¥Ø¶Ø§ÙØ© Ø±Ø¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ø³Ø¬Ù„
        st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()