import streamlit as st
import os
# ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù„ÙŠØ´Ù…Ù„ DirectoryLoader
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.chains import RetrievalQA

# 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Streamlit
st.set_page_config(page_title="ğŸ¤– Ø¨ÙˆØª Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø®ØµØµØ© (RAG)", layout="wide")
st.title("ğŸ’¡ Ø´Ø§Øª Ø¨ÙˆØª Ù…Ø®ØµØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Streamlit Ùˆ Gemini (RAG)")
st.caption("ÙŠØ³Ø£Ù„ ÙˆÙŠØ¬ÙŠØ¨ ÙÙ‚Ø· Ù…Ù† Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø© ÙÙŠ Ù…Ø¬Ù„Ø¯ 'data'.")

# Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…ÙØªØ§Ø­ API (Ø³ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ Ù…Ù† Streamlit Secrets)
if "GEMINI_API_KEY" not in st.secrets:
    st.error("ÙŠØ±Ø¬Ù‰ Ø¥Ø¶Ø§ÙØ© Ù…ÙØªØ§Ø­ Gemini API Ø¥Ù„Ù‰ Streamlit Secrets Ø¨Ø§Ø³Ù… 'GEMINI_API_KEY'")
else:
    # 2. ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Gemini
    llm = GoogleGenerativeAI(
        model="gemini-pro",
        google_api_key=st.secrets["GEMINI_API_KEY"],
        temperature=0.1
    )
    # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…ØªØ¬Ù‡Ø§Øª Ø§Ù„Ø¨Ø­Ø«
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=st.secrets["GEMINI_API_KEY"]
    )

    # 3. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ© (RAG Pipeline)
    try:
        # **Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ù‡Ù†Ø§:** ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ© (.txt) Ù…Ù† Ù…Ø¬Ù„Ø¯ data
        # DirectoryLoader ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø¯Ø¯
        loader = DirectoryLoader(
            './data',             # Ø§Ù„Ù…Ø³Ø§Ø± Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¬Ù„Ø¯
            glob="**/*.txt",      # Ø§Ù„Ù†Ù…Ø·: ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø§Ù…ØªØ¯Ø§Ø¯ .txt
            loader_cls=TextLoader # Ø§Ø³ØªØ®Ø¯Ø§Ù… TextLoader Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙƒÙ†Øµ
        )
        documents = loader.load()

        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ø¨Ø¯Ø¡
        if not documents:
            st.warning("âš ï¸ Ù…Ø¬Ù„Ø¯ 'data' ÙØ§Ø±Øº. ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ Ù…Ù„ÙØ§ØªÙƒ Ø§Ù„Ù†ØµÙŠØ© (.txt) Ø¯Ø§Ø®Ù„Ù‡.")
        else:
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ (Chunks)
            text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
            texts = text_splitter.split_documents(documents)

            # Ø¥Ù†Ø´Ø§Ø¡ Vector Store (Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Chroma
            db = Chroma.from_documents(texts, embeddings)

            # 4. Ø¥Ø¹Ø¯Ø§Ø¯ Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© (RetrievalQA Chain)
            qa = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=db.as_retriever()
            )

            # 5. Ù…Ù†Ø·Ù‚ Ø§Ù„Ø´Ø§Øª Ø¨ÙˆØª ÙÙŠ Streamlit
            if "messages" not in st.session_state:
                st.session_state.messages = []

            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

            if prompt := st.chat_input("Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ù…Ø­ØªÙˆÙ‰ Ù…Ù„ÙØ§ØªÙƒ..."):
                st.session_state.messages.append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    st.markdown(prompt)
                
                # Ø¥Ø±Ø³Ø§Ù„ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…ÙˆØ¬Ù‡ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
                with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª..."):
                    full_prompt = (
                        "Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù… ÙÙ‚Ø·. "
                        "Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ø£Ø¬Ø¨ Ø¨Ù€ 'Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©'."
                        f"\n\nØ§Ù„Ø³Ø¤Ø§Ù„: {prompt}"
                    )
                    
                    response = qa.run(full_prompt)

                st.session_state.messages.append({"role": "assistant", "content": response})
                with st.chat_message("assistant"):
                    st.markdown(response)

    except Exception as e:
        st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
        st.caption("ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø¬Ù„Ø¯ 'data' ÙˆÙ…Ù„ÙØ§Øª .txt Ø¨Ø¯Ø§Ø®Ù„Ù‡ØŒ ÙˆÙ…ÙØªØ§Ø­ API.")
