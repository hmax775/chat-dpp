# -*- coding: utf-8 -*-
# **Knowledge Base Web Chatbot (Streamlit + Gemini API)**
#
# Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù…ØµÙ…Ù… Ù„Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ù…Ù†ØµØ§Øª Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠØ© Ù…Ø«Ù„ Streamlit Cloud.
# ÙŠØªÙ… Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù† Ù…Ø¬Ù„Ø¯ 'data' ÙˆÙŠØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„.
#
# Ù…Ù„Ø§Ø­Ø¸Ø©: ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS ÙƒÙÙ‡Ø±Ø³ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„ØªØ¨Ø³ÙŠØ· Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠ.
#
# Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ØªØ´ØºÙŠÙ„:
# 1. ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª ÙÙŠ requirements.txt
# 2. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ 'data' ÙˆÙˆØ¶Ø¹ Ù…Ù„ÙØ§ØªÙƒ Ø§Ù„Ù€ 400 (Ø¨ØµÙŠØºØ© .txt) Ø¨Ø¯Ø§Ø®Ù„Ù‡.
# 3. ØªÙˆÙÙŠØ± Ù…ÙØªØ§Ø­ Gemini API ÙƒÙ€ "Ù…ØªØºÙŠØ± Ø¨ÙŠØ¦Ø©" (Environment Variable) Ø¨Ø§Ø³Ù… GEMINI_API_KEY
#    Ø¹Ù„Ù‰ Ù…Ù†ØµØ© Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ (Ù…Ø«Ù„ Streamlit Cloud).

import streamlit as st
import os
import faiss
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.faiss import FaissVectorStore

# ============================ 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„Ø«ÙˆØ§Ø¨Øª =============================
DATA_DIR = "./data"

# ============================ 2. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ ==========================

@st.cache_resource(show_spinner="â³ Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹ Ù„Ø­Ø¬Ù… 400 Ù…Ù„Ù)...")
def setup_knowledge_base():
    """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³. ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„ÙÙ‡Ø±Ø³ Ù…Ø¤Ù‚ØªØ§Ù‹ Ù„ØªØ¨Ø³ÙŠØ· Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠ."""
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…ÙØªØ§Ø­ API
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if not gemini_api_key:
        st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ Gemini API. ÙŠØ±Ø¬Ù‰ Ø¥Ø¶Ø§ÙØªÙ‡ ÙƒÙ…ØªØºÙŠØ± Ø¨ÙŠØ¦Ø©.")
        return None

    # 1. Ø¥Ø¹Ø¯Ø§Ø¯ LLM Ùˆ Embedding Model
    Settings.llm = Gemini(model="gemini-2.5-flash", api_key=gemini_api_key, request_timeout=360.0)
    Settings.embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")
    
    st.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬. Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª.")
    
    # 2. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
    try:
        documents = SimpleDirectoryReader(DATA_DIR).load_data()
        st.info(f"ğŸ“„ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(documents)} Ù…Ø³ØªÙ†Ø¯ (Ø¬Ø²Ø¡).")
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù† Ù…Ø¬Ù„Ø¯ 'data': {e}")
        st.warning("ğŸ’¡ ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø¬Ù„Ø¯ 'data' ÙˆÙ…Ù„ÙØ§ØªÙƒ Ø§Ù„Ù†ØµÙŠØ© (.txt) Ø¨Ø¯Ø§Ø®Ù„Ù‡.")
        return None

    if not documents:
        st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø³ØªÙ†Ø¯Ø§Øª. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø´Ø§Øª Ø¨ÙˆØª.")
        return None

    # 3. Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ (Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©)
    # Ù…Ù„Ø§Ø­Ø¸Ø©: Ø³Ù†Ø³ØªØ®Ø¯Ù… FAISS ÙƒØ­Ù„ Ù…Ø¤Ù‚Øª ÙˆÙ…Ø¬Ø§Ù†ÙŠ Ù„Ù„Ù†Ø´Ø± Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØŒ
    # Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„ÙÙ‡Ø±Ø³ ÙÙŠ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ (Memory).
    try:
        # Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS
        dimension = 384  # Ù‡Ø°Ø§ Ù‡Ùˆ Ø­Ø¬Ù… Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª Ù„Ù†Ù…ÙˆØ°Ø¬ all-MiniLM-L6-v2
        faiss_index = faiss.IndexFlatL2(dimension)
        vector_store = FaissVectorStore(faiss_index=faiss_index)

        # Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³
        index = VectorStoreIndex.from_documents(
            documents,
            vector_store=vector_store,
            embed_model=Settings.embed_model,
        )
        st.success("âœ… Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¨Ù†Ø¬Ø§Ø­.")
        return index
    except Exception as e:
        st.error(f"âŒ ÙØ´Ù„ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³: {e}")
        return None

# ============================ 3. ÙˆØ§Ø¬Ù‡Ø© Streamlit ==========================

def main_app():
    st.set_page_config(page_title="Ø´Ø§Øª Ø¨ÙˆØª Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø¤Ø³Ø³ÙŠØ©", layout="wide", initial_sidebar_state="collapsed")
    
    st.markdown("""
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
            html, body, [class*="st-"] { font-family: 'Cairo', sans-serif; text-align: right; direction: rtl; }
            .st-emotion-cache-18ni7ap { padding-top: 1rem; }
        </style>
        """, unsafe_allow_html=True)
        
    st.title("ğŸ¤– Ø´Ø§Øª Ø¨ÙˆØª Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„Ù…Ø¤Ø³Ø³Ø©")
    st.markdown("Ù‡Ø°Ø§ Ø§Ù„Ø´Ø§Øª Ø¨ÙˆØª ÙŠØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„ØªÙƒ **Ø­ØµØ±Ø§Ù‹** Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù€ 400 Ù…Ù„Ù Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ.")

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³
    index = setup_knowledge_base()

    if index is None:
        st.stop()

    # ØªÙ‡ÙŠØ¦Ø© Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
    if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.messages.append({"role": "assistant", "content": "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! Ø£Ù†Ø§ Ø§Ù„Ø´Ø§Øª Ø¨ÙˆØª Ø§Ù„Ù…Ø¤Ø³Ø³ÙŠ. ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø§Ù„ÙŠÙˆÙ…ØŸ"})

    # Ø¹Ø±Ø¶ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    if prompt := st.chat_input("Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§..."):
        # 1. Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # 2. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙˆØªÙ†ÙÙŠØ° RAG
        with st.chat_message("assistant"):
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø®Ø§ØµØ© (RAG)..."):
                try:
                    query_engine = index.as_query_engine(
                        similarity_top_k=5,
                        response_mode="compact"
                    )
                    
                    response = query_engine.query(prompt)

                    # ØµÙŠØ§ØºØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ¥Ø¶Ø§ÙØ© Ø§Ù„Ù…ØµØ§Ø¯Ø±
                    source_names = [Path(node.metadata.get('file_path', 'N/A')).name for node in response.source_nodes if node.metadata.get('file_path')]
                    
                    # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
                    st.markdown(response.response)

                    # Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø±
                    if source_names:
                        sources_markdown = "---  \n **ğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:** \n"
                        sources_markdown += "\n".join([f"- `{name}`" for name in set(source_names)])
                        st.markdown(sources_markdown)
                        
                    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ø¥Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
                    st.session_state.messages.append({"role": "assistant", "content": response.response + "\n\n" + sources_markdown})

                except Exception as e:
                    error_message = f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø¯Ø§Ø®Ù„ÙŠ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ: {e}"
                    st.error(error_message)
                    st.session_state.messages.append({"role": "assistant", "content": error_message})

if __name__ == "__main__":
    main_app()
